{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbed3fc4-1d04-4b0b-af59-4514da11a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /Users/christinaperdue/anaconda3/envs/lab2/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/christinaperdue/anaconda3/envs/lab2/lib/python3.12/site-packages (from lightgbm) (2.1.1)\n",
      "Requirement already satisfied: scipy in /Users/christinaperdue/anaconda3/envs/lab2/lib/python3.12/site-packages (from lightgbm) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb93d1be-ceb7-4658-9c7a-397fee7b1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4e6ae8-255a-47fb-81ab-a748f0db8a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes DataFrame:\n",
      "    0          1\n",
      "0  ID  attribute\n",
      "1   0          l\n",
      "2   1          x\n",
      "3   2          x\n",
      "4   3          x\n",
      "\n",
      "Edges DataFrame:\n",
      "   0  1\n",
      "0  0  5\n",
      "1  0  6\n",
      "2  0  7\n",
      "3  0  8\n",
      "4  0  9\n",
      "\n",
      "Solution Input DataFrame:\n",
      "    0     1     2\n",
      "0  ID  int1  int2\n",
      "1   0    56   396\n",
      "2   1   760   853\n",
      "3   2   340  1137\n",
      "4   3   597   771\n",
      "\n",
      "Cleaned Attributes DataFrame:\n",
      "   Node Attribute\n",
      "1     0         l\n",
      "2     1         x\n",
      "3     2         x\n",
      "4     3         x\n",
      "5     4         x\n",
      "\n",
      "Cleaned Solution Input DataFrame:\n",
      "  ID  Node1  Node2\n",
      "1  0     56    396\n",
      "2  1    760    853\n",
      "3  2    340   1137\n",
      "4  3    597    771\n",
      "5  4   1355   1410\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files\n",
    "attributes_df = pd.read_csv('attributes.csv', header=None)\n",
    "edges_df = pd.read_csv('edges_train.edgelist', header=None)\n",
    "solution_input_df = pd.read_csv('solutionInput.csv', header=None)\n",
    "\n",
    "# Display first few rows of each DataFrame to inspect their structure\n",
    "print(\"Attributes DataFrame:\")\n",
    "print(attributes_df.head())\n",
    "\n",
    "print(\"\\nEdges DataFrame:\")\n",
    "print(edges_df.head())\n",
    "\n",
    "print(\"\\nSolution Input DataFrame:\")\n",
    "print(solution_input_df.head())\n",
    "\n",
    "# Clean the data by removing any headers (if necessary)\n",
    "attributes_df_cleaned = attributes_df.drop(0)\n",
    "solution_input_df_cleaned = solution_input_df.drop(0)\n",
    "\n",
    "# Rename the columns to something meaningful\n",
    "attributes_df_cleaned.columns = ['Node', 'Attribute']\n",
    "solution_input_df_cleaned.columns = ['ID', 'Node1', 'Node2']\n",
    "\n",
    "# Convert the node IDs to integers\n",
    "attributes_df_cleaned['Node'] = attributes_df_cleaned['Node'].astype(int)\n",
    "solution_input_df_cleaned['Node1'] = solution_input_df_cleaned['Node1'].astype(int)\n",
    "solution_input_df_cleaned['Node2'] = solution_input_df_cleaned['Node2'].astype(int)\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"\\nCleaned Attributes DataFrame:\")\n",
    "print(attributes_df_cleaned.head())\n",
    "\n",
    "print(\"\\nCleaned Solution Input DataFrame:\")\n",
    "print(solution_input_df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6db515-332d-4f52-ae4d-19f15acb00e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1500\n",
      "Number of edges: 6600\n",
      "Is the graph connected? True\n",
      "Neighbors of node 0: [np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(12), np.int64(13), np.int64(14), np.int64(16), np.int64(17), np.int64(18), np.int64(21), np.int64(22), np.int64(23), np.int64(26), np.int64(34), np.int64(35), np.int64(40), np.int64(48), np.int64(55), np.int64(57), np.int64(63), np.int64(71), np.int64(81), np.int64(103), np.int64(113), np.int64(115), np.int64(118), np.int64(120), np.int64(124), np.int64(130), np.int64(161), np.int64(162), np.int64(173), np.int64(177), np.int64(178), np.int64(179), np.int64(204), np.int64(213), np.int64(232), np.int64(239), np.int64(242), np.int64(247), np.int64(112), np.int64(218), np.int64(76), np.int64(102), np.int64(323)]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create an undirected graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Add edges from the edges_train.edgelist\n",
    "graph.add_edges_from(edges_df.values)\n",
    "\n",
    "# Print some basic information about the graph\n",
    "print(f\"Number of nodes: {graph.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {graph.number_of_edges()}\")\n",
    "\n",
    "# Check if the graph is connected\n",
    "print(f\"Is the graph connected? {nx.is_connected(graph)}\")\n",
    "\n",
    "# Example of getting neighbors of a node (optional, just to check)\n",
    "node_id = 0  # Example node\n",
    "print(f\"Neighbors of node {node_id}: {list(graph.neighbors(node_id))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf03c5c-e2f1-41f9-a169-8d9b06ac197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Common_Neighbors  Jaccard_Coeff  Adamic_Adar  Preferential_Attachment  \\\n",
      "0                 0        0.00000     0.000000                       30   \n",
      "1                 2        0.04878     0.809981                      222   \n",
      "2                 0        0.00000     0.000000                       49   \n",
      "3                 0        0.00000     0.000000                      105   \n",
      "4                 0        0.00000     0.000000                       35   \n",
      "\n",
      "   Attr_Similarity  \n",
      "0                0  \n",
      "1                1  \n",
      "2                0  \n",
      "3                0  \n",
      "4                1  \n"
     ]
    }
   ],
   "source": [
    "# Function to calculate features for each pair of nodes\n",
    "def calculate_features(graph, node_pairs, attributes_df):\n",
    "    features = []\n",
    "    \n",
    "    # Create a dictionary of attributes for quick lookup\n",
    "    node_attributes = dict(zip(attributes_df['Node'], attributes_df['Attribute']))\n",
    "    \n",
    "    # Iterate over the node pairs\n",
    "    for _, row in node_pairs.iterrows():\n",
    "        node1 = row['Node1']\n",
    "        node2 = row['Node2']\n",
    "        \n",
    "        # Common neighbors\n",
    "        common_neighbors = len(list(nx.common_neighbors(graph, node1, node2)))\n",
    "        \n",
    "        # Jaccard coefficient\n",
    "        jaccard_coeff = next(nx.jaccard_coefficient(graph, [(node1, node2)]), (None, None, 0))[2]\n",
    "        \n",
    "        # Adamic/Adar index\n",
    "        adamic_adar = next(nx.adamic_adar_index(graph, [(node1, node2)]), (None, None, 0))[2]\n",
    "        \n",
    "        # Preferential attachment\n",
    "        preferential_attachment = next(nx.preferential_attachment(graph, [(node1, node2)]), (None, None, 0))[2]\n",
    "        \n",
    "        # Attribute similarity (1 if both nodes have the same attribute, 0 otherwise)\n",
    "        attribute_similarity = 1 if node_attributes.get(node1) == node_attributes.get(node2) else 0\n",
    "        \n",
    "        # Append the calculated features for this pair of nodes\n",
    "        features.append([common_neighbors, jaccard_coeff, adamic_adar, preferential_attachment, attribute_similarity])\n",
    "    \n",
    "    return pd.DataFrame(features, columns=['Common_Neighbors', 'Jaccard_Coeff', 'Adamic_Adar', 'Preferential_Attachment', 'Attr_Similarity'])\n",
    "\n",
    "# Extract features for the node pairs in solutionInput.csv\n",
    "solution_features = calculate_features(graph, solution_input_df_cleaned, attributes_df_cleaned)\n",
    "\n",
    "# Display the extracted features\n",
    "print(solution_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3450f2d-b050-434a-9fe5-31d84ad5ecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5274, number of negative: 10566\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 743\n",
      "[LightGBM] [Info] Number of data points in the train set: 15840, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.332955 -> initscore=-0.694852\n",
      "[LightGBM] [Info] Start training from score -0.694852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Validation Accuracy: 0.8462121212121212\n",
      "Validation Precision: 0.7847498014297061\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_precision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_val, y_pred, labels\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[1;32m     48\u001b[0m disp \u001b[38;5;241m=\u001b[39m ConfusionMatrixDisplay(confusion_matrix\u001b[38;5;241m=\u001b[39mcm,display_labels\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[1;32m     49\u001b[0m disp\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 4.1: Generate positive examples (existing edges)\n",
    "positive_edges_df = pd.DataFrame(edges_df.values, columns=['Node1', 'Node2'])\n",
    "\n",
    "# Step 4.2: Generate negative examples (random non-edges)\n",
    "# We'll sample random pairs of nodes that are not connected\n",
    "all_possible_edges = pd.DataFrame([(i, j) for i in range(graph.number_of_nodes()) for j in range(i+1, graph.number_of_nodes())])\n",
    "existing_edges_set = set(map(tuple, map(sorted, edges_df.values)))\n",
    "\n",
    "# Filter out existing edges to create negative samples\n",
    "negative_edges_df = all_possible_edges[~all_possible_edges.apply(lambda x: tuple(sorted(x)) in existing_edges_set, axis=1)]\n",
    "negative_edges_df = negative_edges_df.sample(n=2*len(positive_edges_df), random_state=42)  # Sample more negative examples (3:1 ratio)\n",
    "\n",
    "# Step 4.3: Extract features for both positive and negative examples\n",
    "positive_features = calculate_features(graph, positive_edges_df, attributes_df_cleaned)\n",
    "negative_features = calculate_features(graph, negative_edges_df.rename(columns={0: 'Node1', 1: 'Node2'}), attributes_df_cleaned)\n",
    "\n",
    "# Step 4.4: Assign labels (1 for positive, 0 for negative)\n",
    "positive_features['Label'] = 1\n",
    "negative_features['Label'] = 0\n",
    "\n",
    "# Combine both into one dataset\n",
    "training_data = pd.concat([positive_features, negative_features])\n",
    "\n",
    "# Step 4.5: Separate features (X) and labels (y)\n",
    "X = training_data.drop(columns=['Label'])\n",
    "y = training_data['Label']\n",
    "\n",
    "# Step 4.6: Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4.7: Train a RandomForest classifier (or LGBM in this case)\n",
    "classifier = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, max_depth=5, n_estimators=50, num_leaves=20, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 4.8: Predict on the validation set and calculate accuracy and precision\n",
    "y_pred = classifier.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "val_precision = precision_score(y_val, y_pred)\n",
    "\n",
    "# Output the validation accuracy, precision, variance, and standard deviation\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred, labels=classifier.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=classifier.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "tp = cm[1][1]\n",
    "tn = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "\n",
    "# Sensitivity (True positive rate) = TP/(TP+FN)\n",
    "sensitivity = tp/(tp+fn)\n",
    "# Specificity (True negative rate) = TN/(TN+FP)\n",
    "specificity = tn/(tn+fp)\n",
    "# False Positive Rate = FP/(FP+TN)\n",
    "fpr = fp/(fp+tn)\n",
    "# False Negative rate = FN/(FN+TP)\n",
    "fnr = fn/(fn+tp)\n",
    "# Precision = TP/(TP+FP)\n",
    "precision = tp/(tp+fp)\n",
    "\n",
    "print('Sensitivity:', sensitivity)\n",
    "print('Specificity:', specificity)\n",
    "print('False positive rate:', fpr)\n",
    "print('False negative rate:', fnr)\n",
    "print('Precision:', precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0c1eb-1245-40d2-918f-f96353d00407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Use the trained classifier to predict on the test set (solutionInput.csv)\n",
    "# The features for the test set (solution_input_df_cleaned) have already been extracted as 'solution_features'\n",
    "test_predictions = classifier.predict(solution_features)\n",
    "\n",
    "# Step 5.2: Add the predictions to the solution input DataFrame\n",
    "solution_input_df_cleaned['Prediction'] = test_predictions\n",
    "\n",
    "# Step 5.3: Prepare the final output DataFrame (only ID and Prediction columns)\n",
    "final_output = solution_input_df_cleaned[['ID', 'Prediction']]\n",
    "\n",
    "# Step 5.4: Save the final output to a CSV file\n",
    "final_output.to_csv('predictions_output.csv', index=False)\n",
    "\n",
    "# Confirm completion\n",
    "print(\"Predictions saved to 'predictions_output.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
